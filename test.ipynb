{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangSmith can be a valuable tool for testing in several ways:\\n\\n1. **Test Case Generation**: LangSmith can be used to generate test cases automatically based on the requirements or specifications of the software being tested. This can help in achieving better test coverage and identifying edge cases that might have been missed during manual test case creation.\\n\\n2. **Test Data Generation**: LangSmith can generate realistic and diverse test data, including edge cases and boundary conditions, which can be used for testing purposes. This can be particularly useful for testing applications that deal with large amounts of data or have complex data structures.\\n\\n3. **Test Script Generation**: LangSmith can generate test scripts in various programming languages, such as Python, Java, or JavaScript, based on the test cases or requirements. These test scripts can then be executed as part of the testing process, either manually or as part of an automated testing framework.\\n\\n4. **Natural Language Test Case Generation**: LangSmith can generate test cases from natural language requirements or user stories. This can be particularly useful in agile development environments, where requirements are often expressed in natural language.\\n\\n5. **Test Oracles**: LangSmith can be used to generate test oracles, which are mechanisms for determining whether a test has passed or failed. This can be particularly useful in situations where the expected output is complex or difficult to specify manually.\\n\\n6. **Test Automation**: LangSmith can be integrated with existing test automation frameworks and tools, such as Selenium or Appium, to automate the testing process. This can help in reducing the time and effort required for testing, as well as improving the consistency and reliability of the testing process.\\n\\n7. **Test Reporting**: LangSmith can generate test reports in various formats, such as HTML or PDF, which can be used to communicate the results of the testing process to stakeholders.\\n\\nOverall, LangSmith can be a powerful tool for testing, particularly in situations where there is a need to generate large amounts of test data or test cases, or where the requirements or specifications are complex or difficult to interpret manually.', response_metadata={'id': 'msg_01FBkyeXUWUQ4rBBuJfgJwSB', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 24, 'output_tokens': 446}}, id='run-9fb33aea-ad06-447b-9f7d-228ee9b9663f-0', usage_metadata={'input_tokens': 24, 'output_tokens': 446, 'total_tokens': 470})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------OPENAI SECTION------------------------------#\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# llm.invoke(\"how can langsmith help with testing?\")\n",
    "#--------------------------OPENAI SECTION------------------------------#\n",
    "\n",
    "#--------------------------TED SECTION------------------------------#\n",
    "# from langchain_community.llms import Ollama\n",
    "\n",
    "# llm = Ollama(\n",
    "#     model=\"llama3:1:70b\",\n",
    "#     base_url=\"https://ted.ins.healthcareintel.com\"\n",
    "# )\n",
    "\n",
    "# llm.invoke(\"hello!\")\n",
    "#--------------------------TED SECTION------------------------------#\n",
    "\n",
    "#--------------------------Claude SECTION------------------------------#\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "#llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0.2, max_tokens=1024)\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0.2, max_tokens=1024, api_key=\"\")\n",
    "#llm.invoke(\"how can langsmith help with testing?\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm \n",
    "\n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "# template = ChatPromptTemplate([\n",
    "#     (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "#     (\"human\", \"Hello, how are you doing?\"),\n",
    "#     (\"ai\", \"I'm doing well, thanks!\"),\n",
    "#     (\"human\", \"{user_input}\"),\n",
    "# ])\n",
    "\n",
    "# prompt_value = template.invoke(\n",
    "#     {\n",
    "#         \"name\": \"Bob\",\n",
    "#         \"user_input\": \"What is your name?\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------Claude SECTION------------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How are you today? Is there something I can help you with or would you like to chat?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"\n",
    ")\n",
    "\n",
    "llm.invoke(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**The Quest for the Perfect Cheese**\\n\\nAs a technical writer, I must emphasize that determining the \"best\" cheese is subjective and often influenced by personal taste preferences. However, we can explore some of the most popular and highly-regarded cheeses in various categories.\\n\\n**Top Contenders:**\\n\\n1. **Parmigiano-Reggiano (Italy)**: Aged for a minimum of 24 months, this granular, crystalline cheese is renowned for its nutty, fruity flavor and crumbly texture.\\n2. **Roquefort (France)**: A classic blue cheese with a pungent, tangy taste and creamy texture, often served as a dessert or used in salad dressings.\\n3. **Mozzarella di Bufala Campana (Italy)**: Fresh, soft, and white, this water buffalo milk-based mozzarella is perfect for pizzas, caprese salads, or snacking.\\n\\n**Special Mentions:**\\n\\n1. **Feta (Greece)**: Salty, crumbly, and delicious, feta is often used in Greek cuisine, from salads to pastries.\\n2. **Goat Gouda (Netherlands)**: Smooth, creamy, and nutty, this Dutch cheese is perfect for snacking or grating over vegetables.\\n\\n**Honorable Mentions:**\\n\\n1. **Cheddar (UK/USA)**: A versatile, popular choice with a range of flavors from mild to extra-sharp.\\n2. **Brie (France)**: Soft-ripened and creamy, brie is often baked or served with crackers for a rich snack.\\n\\n**The Verdict:**\\n\\nWhile it\\'s difficult to crown a single \"best\" cheese, these top contenders showcase the incredible diversity of flavors and textures in the world of cheese. Feel free to explore and find your own perfect match!\\n\\nWould you like more information on any specific type of cheese or its usage?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------LLM Chain & Prompt Templates-----------\n",
    "# we can guide responses with a prompt template\n",
    "# prompt templates convert raw user input to better input to the LLM\n",
    "# Chat Prompt Templates: https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "# deeper dive to into the chain part: https://python.langchain.com/v0.1/docs/modules/model_io/\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a technical writeer\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# combine the prompt and the model initialization into \"a simple LLM chain\"\n",
    "# the output of a ChatModel (and therefore this chain) is a message\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"input: what is the best cheese?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**The Best Cheese: A Technical Perspective**\\n\\nAs a technical writer, I must emphasize that the notion of \"the best\" cheese is subjective and can vary depending on personal taste preferences, culinary applications, and cultural traditions. However, I can provide an overview of popular cheese types, their characteristics, and uses to help you make an informed decision.\\n\\n**Classification of Cheeses**\\n\\nCheeses can be broadly categorized into six main types based on their texture, moisture content, and production methods:\\n\\n1. **Fresh Cheeses**: Soft, creamy, and often spreadable, examples include Ricotta, Cottage Cheese, and Cream Cheese.\\n2. **Soft-Ripened Cheeses**: Aged to a soft, white interior with a bloomy or washed rind, examples include Brie, Camembert, and Feta.\\n3. **Semi-Soft Cheeses**: A balance of texture and flavor, examples include Gouda, Edam, and Manchego.\\n4. **Semi-Hard Cheeses**: Firm, dense, and often aged for a longer period, examples include Cheddar, Swiss, and Parmesan.\\n5. **Hard Cheeses**: Very dense and aged for an extended time, examples include Grana Padano, Pecorino Romano, and Asiago.\\n6. **Blue Cheeses**: Infused with mold cultures, resulting in a strong, pungent flavor, examples include Roquefort, Stilton, and Gorgonzola.\\n\\n**Considerations for the \"Best\" Cheese**\\n\\nWhen evaluating cheese, consider factors such as:\\n\\n* **Flavor profile**: Do you prefer mild and creamy or strong and pungent?\\n* **Texture**: Soft and spreadable, semi-soft, or firm and crumbly?\\n* **Culinary application**: Cheese plates, cooking, baking, or snacking?\\n* **Production methods**: Traditional vs. modern; raw milk vs. pasteurized?\\n\\n**Top Contenders for the \"Best\" Cheese**\\n\\nBased on popularity and versatility, here are some top contenders:\\n\\n1. **Parmesan**: Aged for a minimum of 24 months, this Italian classic is perfect for snacking, grating over pasta dishes, or adding to soups.\\n2. **Cheddar**: With its rich, tangy flavor, Cheddar is a popular choice for snacking, grilled cheese sandwiches, and mac \\'n cheese.\\n3. **Mozzarella**: Soft, white, and creamy, Mozzarella is ideal for pizzas, caprese salads, and lasagnas.\\n\\n**Conclusion**\\n\\nWhile there\\'s no definitive answer to the \"best\" cheese, understanding the different types of cheeses and their characteristics can help you make an informed decision based on your personal preferences and needs. Experiment with various cheeses to find your perfect match!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------String Output Parser-----------\n",
    "# https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a technical writeer\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# its often more convenient to work with strings (instead of the message output from the block above)\n",
    "# here we add a simple output parser to our chain to convert the chat message to a string\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# invoke the chain, now the answer will be a string (instead of a ChatMessage)\n",
    "chain.invoke({\"input: what is the best cheese?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6212705969810486, 1.2343552112579346, 2.0774543285369873, -0.7935279011726379, 0.789751708507537\n",
      "[-1.284210205078125, 1.4845789670944214, 1.4499554634094238, -0.6983569264411926, 0.2896924912929535\n",
      "[0.22061067819595337, 0.6924100518226624, 0.41823121905326843, -0.5657470226287842, 0.17675109207630\n"
     ]
    }
   ],
   "source": [
    "#-----------LangChain's OllamaEmbeddings Class-----------\n",
    "# https://ollama.com/blog/embedding-models\n",
    "# https://python.langchain.com/v0.2/docs/integrations/text_embedding/ollama/\n",
    "# embedding models are often used in retrieval-augmented generation (RAG) flows\n",
    "# both as a part of indexing data as well as later retrieving it\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "#---------step 1: Instantiation---------#\n",
    "# instantiate our model object and generate embeddings\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"#,\n",
    ")\n",
    "#---------end: Instantiation---------#\n",
    "\n",
    "#---------step 2: indexing & retrieval---------#\n",
    "# below, see how to index and retrieve data using the embeddings object we initialized above\n",
    "# in this example we'll index and retrieve a sample document in the \"InMemoryVectorStore\"\n",
    "\n",
    "# Create a vector store with a sample text\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    [text],\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "# Use the vectorstore as a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Retrieve the most similar text\n",
    "retrieved_documents = retriever.invoke(\"What is LangChain?\")\n",
    "\n",
    "# show the retrieved document's content\n",
    "retrieved_documents[0].page_content\n",
    "\n",
    "#---------end: indexing & retrieval---------#\n",
    "\n",
    "#---------step 3: direct usage---------#\n",
    "# under the hood, the vectorstore and retriever implementations are calling\n",
    "# \"embeddings.embed_documents(...)\" and \"embeddings.embed_query(...)\" to create embeddings for the text(s) used in \"from_text\" and retrieval \"invoke\"\n",
    "# you can directly call these methods to get embeddings for your own use cases\n",
    "\n",
    "# you can embed single texts or documents with embed_query\n",
    "single_vector = embedding_model.embed_query(text)\n",
    "print(str(single_vector)[:100])  # Show the first 100 characters of the vector\n",
    "\n",
    "text2 = (\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\n",
    ")\n",
    "\n",
    "# embed multiple texts with embed_documents\n",
    "two_vectors = embedding_model.embed_documents([text, text2])\n",
    "for vector in two_vectors:\n",
    "    print(str(vector)[:100])  # Show the first 100 characters of the vector\n",
    "#---------end: direct usage---------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Ollama Embeddings-----------\n",
    "# https://ollama.com/blog/embedding-models\n",
    "# https://python.langchain.com/v0.2/docs/integrations/text_embedding/ollama/\n",
    "# embedding models are often used in retrieval-augmented generation (RAG) flows\n",
    "# both as a part of indexing data as well as later retrieving it\n",
    "import ollama\n",
    "import chromadb\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# Create a vector store with a sample text\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# instantiate our model object & generate embeddings\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"#,\n",
    ")\n",
    "\n",
    "#---------step 1: generate embeddings---------#\n",
    "documents = [\n",
    "  \"Llamas are members of the camelid family meaning they're pretty closely related to vicuñas and camels\",\n",
    "  \"Llamas were first domesticated and used as pack animals 4,000 to 5,000 years ago in the Peruvian highlands\",\n",
    "  \"Llamas can grow as much as 6 feet tall though the average llama between 5 feet 6 inches and 5 feet 9 inches tall\",\n",
    "  \"Llamas weigh between 280 and 450 pounds and can carry 25 to 30 percent of their body weight\",\n",
    "  \"Llamas are vegetarians and have very efficient digestive systems\",\n",
    "  \"Llamas live to be about 20 years old, though some only live for 15 years and others live to be 30 years old\",\n",
    "]\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"docs\")\n",
    "\n",
    "# store each document in a vector embedding database\n",
    "for i, d in enumerate(documents):\n",
    "  response = ollama.embeddings(model=\"mxbai-embed-large\", prompt=d)\n",
    "  embedding = response[\"embedding\"]\n",
    "  collection.add(\n",
    "    ids=[str(i)],\n",
    "    embeddings=[embedding],\n",
    "    documents=[d]\n",
    "  )\n",
    "#---------end: generate embeddings---------#\n",
    "\n",
    "#---------step 2: retrieve---------#\n",
    "# an example prompt\n",
    "prompt = \"What animals are llamas related to?\"\n",
    "\n",
    "# generate an embedding for the prompt and retrieve the most relevant doc\n",
    "response = ollama.embeddings(\n",
    "  prompt=prompt,\n",
    "  model=\"mxbai-embed-large\"\n",
    ")\n",
    "results = collection.query(\n",
    "  query_embeddings=[response[\"embedding\"]],\n",
    "  n_results=1\n",
    ")\n",
    "data = results['documents'][0][0]\n",
    "#---------end: retrieve---------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<coroutine object Embeddings.aembed_documents at 0x000002DBE51E8040>\n"
     ]
    }
   ],
   "source": [
    "#-----------text embeddings & embedding models-----------\n",
    "# LangChain embedddings: https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/\n",
    "# Ollama embeddings: https://ollama.com/blog/embedding-models\n",
    "# api: https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.ollama.OllamaEmbeddings.html\n",
    "# embedding models are models that are trained specifically to generate vector embeddings\n",
    "# vector embeddings - long arrays of numbers that represent semantic meaning for a given sequence of text\n",
    "# the resulting vector embedding arrays can then be stored in a database\n",
    "# then whatever looks in the database compares the vector embeddings as a way to search for data that has a similiar meaning to the user prompt\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"#,\n",
    "    #embed_instruction=\"Focus on extracting the sentiment from the following text:\"\n",
    ")\n",
    "\n",
    "text = \"The product was fantastic and exceeded all my expectations!\"\n",
    "\n",
    "embedding = embedding_model.aembed_documents(text)\n",
    "\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Vector Stores-----------\n",
    "# https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "embeddings = OllamaEmbeddings()\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Best Cheese: A Technical Analysis\\n\\nAbstract:\\nWith over 1,000 types of cheese available worldwide, selecting the \"best\" cheese can be a daunting task. This report provides an objective analysis of various cheeses, considering factors such as flavor profile, texture, and versatility.\\n\\nIntroduction:\\nCheese is a nutrient-rich food product obtained from the proteins and fats in milk. The diversity of cheese styles and flavors has led to a multitude of applications in cuisine, ranging from pizzas to salads. However, with so many options available, it\\'s challenging to pinpoint a single \"best\" cheese.\\n\\nMethodology:\\nTo determine the best cheese, we evaluated 20 popular varieties based on three key parameters:\\n\\n1. **Flavor Profile**: A subjective assessment of each cheese\\'s taste, aroma, and overall gastronomic experience.\\n2. **Texture**: An evaluation of the cheese\\'s mouthfeel, crumbliness, or creaminess.\\n3. **Versatility**: The ability of each cheese to be used in various dishes, from appetizers to desserts.\\n\\nResults:\\n\\n**Top 5 Cheeses**\\n\\n1. **Parmesan**: With a score of 9.2/10, Parmesan emerged as the top contender. Its nutty flavor profile, crumbly texture, and versatility in Italian cuisine (e.g., pasta, pizza) make it an excellent all-around choice.\\n2. **Cheddar**: Scoring 8.8/10, Cheddar\\'s sharpness, creaminess, and adaptability in a range of applications (e.g., mac \\'n cheese, grilled cheese) earned it the second spot.\\n3. **Mozzarella**: With a score of 8.5/10, Mozzarella\\'s mild flavor, smooth texture, and essential role in Neapolitan pizza secured its place among the top three.\\n4. **Feta**: Feta\\'s salty, tangy taste (8.2/10) and crumbly texture made it an excellent addition to salads, pasta dishes, and Mediterranean cuisine.\\n5. **Gouda**: Scoring 8.1/10, Gouda\\'s rich flavor profile, creamy texture, and wide range of uses (e.g., grilled cheese, sauces) rounded out the top five.\\n\\nConclusion:\\nWhile opinions may vary, our technical analysis revealed that Parmesan is the most well-rounded and versatile cheese among those evaluated. Its unique combination of nutty flavors, crumbly texture, and adaptability in various dishes make it an excellent choice for any culinary application.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------Retrieval Chain-----------\n",
    "# retrieval is useful when you have too much data to pass to the LLM directly\n",
    "# you can use a retriever to fetch only the most relevant pieces and pass those in\n",
    "# in this process, we will look up relevant documents from a \"Retriever\" and then pass them into the prompt\n",
    "# a retriever can be backed by anything (SQL table, the internet, etc.)\n",
    "# in this instance we will populate a vector store & use that as a retriever\n",
    "# vector stores: https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n",
    "# WebBaseLoader: https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html\n",
    "# LangChain embedding models: https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/\n",
    "# Ollama embedding models API docs: https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.ollama.OllamaEmbeddings.html\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "embeddings = OllamaEmbeddings()\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a technical writeer\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"input: what is the best cheese?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
