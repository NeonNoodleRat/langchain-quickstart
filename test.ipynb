{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangSmith can be a valuable tool for testing in several ways:\\n\\n1. **Test Case Generation**: LangSmith can be used to generate test cases automatically based on the requirements or specifications of the software being tested. This can help in achieving better test coverage and identifying edge cases that might have been missed during manual test case creation.\\n\\n2. **Test Data Generation**: LangSmith can generate realistic and diverse test data, including edge cases and boundary conditions, which can be used for testing purposes. This can be particularly useful for testing applications that deal with large amounts of data or have complex data structures.\\n\\n3. **Test Script Generation**: LangSmith can generate test scripts in various programming languages, such as Python, Java, or JavaScript, based on the test cases or requirements. These test scripts can then be executed as part of the testing process, either manually or as part of an automated testing framework.\\n\\n4. **Natural Language Test Case Generation**: LangSmith can generate test cases from natural language requirements or user stories. This can be particularly useful in agile development environments, where requirements are often expressed in natural language.\\n\\n5. **Test Oracles**: LangSmith can be used to generate test oracles, which are mechanisms for determining whether a test has passed or failed. This can be particularly useful in situations where the expected output is complex or difficult to specify manually.\\n\\n6. **Test Automation**: LangSmith can be integrated with existing test automation frameworks and tools, such as Selenium or Appium, to automate the testing process. This can help in reducing the time and effort required for testing, as well as improving the consistency and reliability of the testing process.\\n\\n7. **Test Reporting**: LangSmith can generate test reports in various formats, such as HTML or PDF, which can be used to communicate the results of the testing process to stakeholders.\\n\\nOverall, LangSmith can be a powerful tool for testing, particularly in situations where there is a need to generate large amounts of test data or test cases, or where the requirements or specifications are complex or difficult to interpret manually.', response_metadata={'id': 'msg_01FBkyeXUWUQ4rBBuJfgJwSB', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 24, 'output_tokens': 446}}, id='run-9fb33aea-ad06-447b-9f7d-228ee9b9663f-0', usage_metadata={'input_tokens': 24, 'output_tokens': 446, 'total_tokens': 470})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------OPENAI SECTION------------------------------#\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# llm.invoke(\"how can langsmith help with testing?\")\n",
    "#--------------------------OPENAI SECTION------------------------------#\n",
    "\n",
    "#--------------------------TED SECTION------------------------------#\n",
    "# from langchain_community.llms import Ollama\n",
    "\n",
    "# llm = Ollama(\n",
    "#     model=\"llama3:1:70b\",\n",
    "#     base_url=\"https://ted.ins.healthcareintel.com\"\n",
    "# )\n",
    "\n",
    "# llm.invoke(\"hello!\")\n",
    "#--------------------------TED SECTION------------------------------#\n",
    "\n",
    "#--------------------------Claude SECTION------------------------------#\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "#llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0.2, max_tokens=1024)\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0.2, max_tokens=1024, api_key=\"\")\n",
    "#llm.invoke(\"how can langsmith help with testing?\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm \n",
    "\n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "# template = ChatPromptTemplate([\n",
    "#     (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "#     (\"human\", \"Hello, how are you doing?\"),\n",
    "#     (\"ai\", \"I'm doing well, thanks!\"),\n",
    "#     (\"human\", \"{user_input}\"),\n",
    "# ])\n",
    "\n",
    "# prompt_value = template.invoke(\n",
    "#     {\n",
    "#         \"name\": \"Bob\",\n",
    "#         \"user_input\": \"What is your name?\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------Claude SECTION------------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test to make sure you can connect to ted\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"\n",
    ")\n",
    "\n",
    "llm.invoke(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**The Age-Old Question: What is the Best Cheese?**\\n\\nAs a connoisseur of all things dairy, I\\'m excited to delve into the world of artisanal and farmstead cheeses. With over 1,000 varieties to choose from, selecting the \"best\" cheese can be a daunting task. However, after careful consideration, I\\'d like to propose a few exceptional contenders that are sure to please even the most discerning palates.\\n\\n**Top Contenders:**\\n\\n1. **Parmigiano-Reggiano**: This Italian classic is often regarded as the king of cheeses. Aged for a minimum of 24 months, its nutty, fruity flavor profile and crumbly texture make it an ideal addition to pasta dishes, salads, or enjoyed on its own.\\n2. **Roquefort**: Hailing from France, this iconic blue cheese boasts a rich, tangy flavor with hints of sweet caramel and toasted nuts. Its creamy texture and pungent aroma make it perfect for salad dressings, sauces, or as a bold accompaniment to fruit and crackers.\\n3. **Manchego**: This Spanish sheep\\'s milk cheese is known for its firm, nutty texture and subtle sweetness. Aged for at least 6 months, Manchego pairs beautifully with membrillo (quince paste) and cured meats, making it an excellent choice for tapas or snacking.\\n\\n**Honorable Mentions:**\\n\\n* **Gouda**: A mild, creamy Dutch cheese perfect for snacking or grating.\\n* **Brie**: A soft, buttery French cheese ideal for baking or serving with fruit.\\n* **Feta**: A tangy, crumbly Greek cheese great for salads, pastries, or as a topping.\\n\\n**Conclusion:**\\n\\nWhile it\\'s challenging to crown a single \"best\" cheese, these exceptional varieties are sure to impress even the most discerning palates. Whether you\\'re a fan of sharp and pungent or mild and creamy, there\\'s a perfect cheese waiting for you.\\n\\nWhat\\'s your favorite type of cheese? Do you have a go-to variety or a special occasion cheese? Share with me in the comments!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------LLM Chain & Prompt Templates-----------\n",
    "# we can guide responses with a prompt template\n",
    "# prompt templates convert raw user input to better input to the LLM\n",
    "# Chat Prompt Templates: https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "# deeper dive to into the chain part: https://python.langchain.com/v0.1/docs/modules/model_io/\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a technical writeer\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# combine the prompt and the model initialization into \"a simple LLM chain\"\n",
    "# the output of a ChatModel (and therefore this chain) is a message\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"input: what is the best cheese?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**The Best Cheese: A Technical Perspective**\\n\\nAs a technical writer, I must emphasize that the notion of \"the best\" cheese is subjective and can vary depending on personal taste preferences, culinary applications, and cultural traditions. However, I can provide an overview of popular cheese types, their characteristics, and uses to help you make an informed decision.\\n\\n**Classification of Cheeses**\\n\\nCheeses can be broadly categorized into six main types based on their texture, moisture content, and production methods:\\n\\n1. **Fresh Cheeses**: Soft, creamy, and often spreadable, examples include Ricotta, Cottage Cheese, and Cream Cheese.\\n2. **Soft-Ripened Cheeses**: Aged to a soft, white interior with a bloomy or washed rind, examples include Brie, Camembert, and Feta.\\n3. **Semi-Soft Cheeses**: A balance of texture and flavor, examples include Gouda, Edam, and Manchego.\\n4. **Semi-Hard Cheeses**: Firm, dense, and often aged for a longer period, examples include Cheddar, Swiss, and Parmesan.\\n5. **Hard Cheeses**: Very dense and aged for an extended time, examples include Grana Padano, Pecorino Romano, and Asiago.\\n6. **Blue Cheeses**: Infused with mold cultures, resulting in a strong, pungent flavor, examples include Roquefort, Stilton, and Gorgonzola.\\n\\n**Considerations for the \"Best\" Cheese**\\n\\nWhen evaluating cheese, consider factors such as:\\n\\n* **Flavor profile**: Do you prefer mild and creamy or strong and pungent?\\n* **Texture**: Soft and spreadable, semi-soft, or firm and crumbly?\\n* **Culinary application**: Cheese plates, cooking, baking, or snacking?\\n* **Production methods**: Traditional vs. modern; raw milk vs. pasteurized?\\n\\n**Top Contenders for the \"Best\" Cheese**\\n\\nBased on popularity and versatility, here are some top contenders:\\n\\n1. **Parmesan**: Aged for a minimum of 24 months, this Italian classic is perfect for snacking, grating over pasta dishes, or adding to soups.\\n2. **Cheddar**: With its rich, tangy flavor, Cheddar is a popular choice for snacking, grilled cheese sandwiches, and mac \\'n cheese.\\n3. **Mozzarella**: Soft, white, and creamy, Mozzarella is ideal for pizzas, caprese salads, and lasagnas.\\n\\n**Conclusion**\\n\\nWhile there\\'s no definitive answer to the \"best\" cheese, understanding the different types of cheeses and their characteristics can help you make an informed decision based on your personal preferences and needs. Experiment with various cheeses to find your perfect match!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------String Output Parser-----------\n",
    "# https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a technical writeer\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# its often more convenient to work with strings (instead of the message output from the block above)\n",
    "# here we add a simple output parser to our chain to convert the chat message to a string\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# invoke the chain, now the answer will be a string (instead of a ChatMessage)\n",
    "chain.invoke({\"input: what is the best cheese?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6212705969810486, 1.2343552112579346, 2.0774543285369873, -0.7935279011726379, 0.789751708507537\n",
      "[-1.284210205078125, 1.4845789670944214, 1.4499554634094238, -0.6983569264411926, 0.2896924912929535\n",
      "[0.22061067819595337, 0.6924100518226624, 0.41823121905326843, -0.5657470226287842, 0.17675109207630\n"
     ]
    }
   ],
   "source": [
    "#-----------LangChain's OllamaEmbeddings Class-----------\n",
    "# https://python.langchain.com/v0.2/docs/integrations/text_embedding/ollama/\n",
    "# embedding models are often used in retrieval-augmented generation (RAG) flows\n",
    "# both as a part of indexing data as well as later retrieving it\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "#---------step 1: Instantiation---------#\n",
    "# instantiate our model object and generate embeddings\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"#,\n",
    ")\n",
    "#---------end: Instantiation---------#\n",
    "\n",
    "#---------step 2: indexing & retrieval---------#\n",
    "# below, see how to index and retrieve data using the embeddings object we initialized above\n",
    "# in this example we'll index and retrieve a sample document in the \"InMemoryVectorStore\"\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "\n",
    "# create a vector store with a sample text\n",
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    [text],\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "# Use the vectorstore as a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Retrieve the most similar text\n",
    "retrieved_documents = retriever.invoke(\"What is LangChain?\")\n",
    "\n",
    "# show the retrieved document's content\n",
    "retrieved_documents[0].page_content\n",
    "\n",
    "#---------end: indexing & retrieval---------#\n",
    "\n",
    "#---------step 3: direct usage---------#\n",
    "# under the hood, the vectorstore and retriever implementations are calling\n",
    "# \"embeddings.embed_documents(...)\" and \"embeddings.embed_query(...)\" to create embeddings for the text(s) used in \"from_text\" and retrieval \"invoke\"\n",
    "# you can directly call these methods to get embeddings for your own use cases\n",
    "\n",
    "# you can embed single texts or documents with embed_query\n",
    "single_vector = embedding_model.embed_query(text)\n",
    "print(str(single_vector)[:100])  # Show the first 100 characters of the vector\n",
    "\n",
    "text2 = (\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\n",
    ")\n",
    "\n",
    "# embed multiple texts with embed_documents\n",
    "two_vectors = embedding_model.embed_documents([text, text2])\n",
    "for vector in two_vectors:\n",
    "    print(str(vector)[:100])  # Show the first 100 characters of the vector\n",
    "#---------end: direct usage---------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS ONE IS CURRENTLY BROKEN\n",
    "\n",
    "#-----------Ollama Embeddings-----------\n",
    "# https://ollama.com/blog/embedding-models\n",
    "# https://python.langchain.com/v0.2/docs/integrations/text_embedding/ollama/\n",
    "# embedding models are often used in retrieval-augmented generation (RAG) flows\n",
    "# both as a part of indexing data as well as later retrieving it\n",
    "import ollama\n",
    "import chromadb\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# Create a vector store with a sample text\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# instantiate our embedding model object \n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"#,\n",
    ")\n",
    "\n",
    "#---------step 1: generate embeddings---------#\n",
    "documents = [\n",
    "  \"Llamas are members of the camelid family meaning they're pretty closely related to vicu√±as and camels\",\n",
    "  \"Llamas were first domesticated and used as pack animals 4,000 to 5,000 years ago in the Peruvian highlands\",\n",
    "  \"Llamas can grow as much as 6 feet tall though the average llama between 5 feet 6 inches and 5 feet 9 inches tall\",\n",
    "  \"Llamas weigh between 280 and 450 pounds and can carry 25 to 30 percent of their body weight\",\n",
    "  \"Llamas are vegetarians and have very efficient digestive systems\",\n",
    "  \"Llamas live to be about 20 years old, though some only live for 15 years and others live to be 30 years old\",\n",
    "]\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"docs\")\n",
    "\n",
    "# store each document in a vector embedding database\n",
    "for i, d in enumerate(documents):\n",
    "  response = ollama.embeddings(model=\"mxbai-embed-large\", prompt=d)\n",
    "  embedding = response[\"embedding\"]\n",
    "  collection.add(\n",
    "    ids=[str(i)],\n",
    "    embeddings=[embedding],\n",
    "    documents=[d]\n",
    "  )\n",
    "#---------end: generate embeddings---------#\n",
    "\n",
    "#---------step 2: retrieve---------#\n",
    "# an example prompt\n",
    "prompt = \"What animals are llamas related to?\"\n",
    "\n",
    "# generate an embedding for the prompt and retrieve the most relevant doc\n",
    "response = ollama.embeddings(\n",
    "  prompt=prompt,\n",
    "  model=\"mxbai-embed-large\"\n",
    ")\n",
    "results = collection.query(\n",
    "  query_embeddings=[response[\"embedding\"]],\n",
    "  n_results=1\n",
    ")\n",
    "data = results['documents'][0][0]\n",
    "#---------end: retrieve---------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<coroutine object Embeddings.aembed_documents at 0x000002DBE51E8040>\n"
     ]
    }
   ],
   "source": [
    "#-----------text embeddings & embedding models-----------\n",
    "# LangChain embedddings: https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/\n",
    "# Ollama embeddings: https://ollama.com/blog/embedding-models\n",
    "# api: https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.ollama.OllamaEmbeddings.html\n",
    "# embedding models are models that are trained specifically to generate vector embeddings\n",
    "# vector embeddings - long arrays of numbers that represent semantic meaning for a given sequence of text\n",
    "# the resulting vector embedding arrays can then be stored in a database\n",
    "# then whatever looks in the database compares the vector embeddings as a way to search for data that has a similiar meaning to the user prompt\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"#,\n",
    "    #embed_instruction=\"Focus on extracting the sentiment from the following text:\"\n",
    ")\n",
    "\n",
    "text = \"The product was fantastic and exceeded all my expectations!\"\n",
    "\n",
    "embedding = embedding_model.aembed_documents(text)\n",
    "\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing in several ways:\n",
      "\n",
      "1. **Creating datasets**: Developers can create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\n",
      "2. **Running custom evaluations**: LangSmith allows developers to run custom evaluations (both LLM and heuristic-based) to score test results.\n",
      "3. **Comparison View**: The Comparison View feature enables users to view results for different configurations on the same datapoints side-by-side, making it easier to track and diagnose regressions in test scores across multiple revisions of the application.\n",
      "4. **Automated testing**: LangSmith provides automations that can be used to automatically score traces, send them to annotation queues, or send them to datasets, which can help with automated testing.\n",
      "5. **Playground environment**: The playground environment allows developers to quickly test out different prompts and models, and every run is logged in the system, making it easier to create test cases or compare with other runs.\n",
      "\n",
      "Overall, LangSmith provides a range of features that can help with testing LLM applications, including data-driven testing, automated testing, and exploratory testing.\n"
     ]
    }
   ],
   "source": [
    "#-----------Vector Stores & Retrieval Chain-----------\n",
    "# https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n",
    "# retrieval is useful when you have too much data to pass to the LLM directly\n",
    "# you can use a retriever to fetch only the most relevant pieces and pass those in\n",
    "# in this process, we will look up relevant documents from a \"Retriever\" and then pass them into the prompt\n",
    "# a retriever can be backed by anything (SQL table, the internet, etc.)\n",
    "# in this instance we will populate a vector store & use that as a retriever\n",
    "# vector stores: https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n",
    "# WebBaseLoader: https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html\n",
    "# LangChain embedding models: https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/\n",
    "# Ollama embedding models API docs: https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.ollama.OllamaEmbeddings.html\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"\n",
    ")\n",
    "\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"#,\n",
    "    #embed_instruction=\"Focus on extracting the sentiment from the following text:\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "#embeddings = OllamaEmbeddings()\n",
    "embeddings = embedding_model\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**The Age-Old Question: What is the Best Cheese?**\\n\\nAs a technical writer, I must approach this inquiry with a critical and analytical mindset. The answer, much like the perfect grilled cheese sandwich, depends on various factors and personal preferences.\\n\\nFrom a technical standpoint, there are over 1,000 types of cheese produced globally, each with its unique characteristics, textures, and flavor profiles. To narrow down the options, let\\'s consider some popular categories:\\n\\n1. **Soft Cheeses**: Brie, Feta, Goat Cheese, and Camembert are renowned for their creamy textures and mild flavors.\\n2. **Hard Cheeses**: Parmesan, Cheddar, Swiss, and Gouda are prized for their rich, nutty flavors and firm textures.\\n3. **Blue Cheeses**: Gorgonzola, Stilton, and Roquefort are celebrated for their pungent, tangy flavors and distinctive veining.\\n4. **Fresh Cheeses**: Mozzarella, Ricotta, and Cottage Cheese are favored for their mild flavors and soft, spreadable textures.\\n\\n**The Verdict:**\\n\\nWhile it\\'s challenging to pinpoint a single \"best\" cheese, some of the most popular and versatile options include:\\n\\n* Parmesan (Italy): A classic hard cheese with a nutty flavor, perfect for grating or slicing.\\n* Cheddar (England): A mild to sharp hard cheese, ideal for snacking, grating, or melting.\\n* Mozzarella (Italy): A soft, milky fresh cheese, great for pizzas, caprese salads, and lasagnas.\\n\\n**Conclusion:**\\n\\nThe \"best\" cheese ultimately depends on personal taste preferences, recipe requirements, and cultural traditions. Experiment with different types, textures, and flavors to discover your perfect match. Whether you\\'re a cheese aficionado or just starting your dairy journey, there\\'s sure to be a delightful world of cheese waiting for you!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------Conversation Retrieval Chain-----------\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"\n",
    ")\n",
    "\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"llama3.1:70b\",\n",
    "    base_url=\"http://ted.ins.healthcareintel.com\"#,\n",
    "    #embed_instruction=\"Focus on extracting the sentiment from the following text:\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "#embeddings = OllamaEmbeddings()\n",
    "embeddings = embedding_model\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
